# Introduction

`Perfsis` is a portmanteau word made from the first syllable of `performance` and the last of `analysis`.
Performance analysis is a big topic; perfsis is a tool for doing many (but not all) kinds of performance analysis on d8m programs. The focus to date has been on running times, but memory usage measurements can be done within its framework. The graphical UI part also lets you compare d8m programs with programs written in other programming languages, via an ability to show _external dataseries_. It is well suited to studying variations in runtime environment (CPU, memory, etc), compiler versions, and so on.

Perfsis is written in d8m. Architecturally, it consists of a _back end_, called _perfsisMeasure_, that collects and organizes timing data, and a _front end_, called _perfsisShow_, that provides a graphical UI for display and analysis of that information. PerfsisShow runs on a browser connected to a localhost port; it is built using another d8m tool called webgen. In other words, perfsisShow is implemented as a web server and client, but run locally with access to the files containing performance data generated by perfsisMeasure.

To use perfsisMeasure, you present a specification file (called, naturally, a _perfspec_) that provides a _load model_ for a _program under test_. The load model sets up the program, provides whatever data it needs, calls the various operations you want to measure, and specifies how to do the measurements. Typically, the load model provides invented data using a PRNG (a pseudo-random number generator), but load models can use whatever data source best suits your needs. A perfspec is essentially a d8m program but it has extra syntax that lets you declare parameters, how you want them to vary, and the outputs you're measuring. These declarations let you provide enough information about the parameters and outputs that you or someone else should be able to make sense of the data in the frontend without further intervention. Naturally, this works best if you fill in the relevant fields in a thoughtful way.

Given a perfspec, perfsisMeasure reads and analyzes it, then generates one or more d8m programs that instantiate the load model and program under test, possibly in several different ways that you've specified. And it generates a _driver_ program that can run these programs with the parameter values you've specified, collect the data they generate, and organize everything into a set of files understood by perfsisShow. You run the driver, which runs other programs as directed.

# Generating Data

We start with the specification, a _perfspec file_. It should have extension `perfspec`, so its name is `something.perfspec`.
Given such a file, perfsisMeasure is going to create a directory named `wd.something`. We'll call this the `wd` or working directory.
The working directory will contain the load model and driver programs generated from the perfspec,
binary files associated with these program files, a couple of json files, one or more subdirectories that name different
versions of the data (named 0, 1, 2, etc), and possibly subdirectories named `externals` and `savedCharts`.
Each version subdirectory contains a .csv file for each generated load model.
We'll see a bit later where different versions come from.
The short form is that versions can reflect any kinds of differences you care about &mdash; in the runtime environment,
the compiler, different versions of the load model or program under test, etc.
You describe the "context" for a set of measurements when you create a version.

As stated in the introduction, a perfspec file is a d8m source file augmented with extra statements needed for performance analysis.
The focus in this section is describing those extra statements.
The features start at the very beginning of your perfspec file: any `//` comments included at the
beginning of the file will get copied (with the `//` prefixes removed) into a meta information
file where perfsisShow can show them to users. So it's a good idea to start perfspec files with some
comments explaining what the perfspec does and how it works. 

The syntax additions to standard d8m are the following statement types:

- `parameter`
- `output`
- `query`
- `substitute`

Each statement consists of the keyword, which must be the first token on a line, then arguments which are generally comma-separated lists of string literals, although the details vary. Each statement terminates after one or more complete lines, and the standard d8m code defining the program and load model continues on the following line.

We make some restrictions in the syntax of the extended statements to simplify the translation process for perfsisMeasure.
Using string literals as arguments avoids the need for a full d8m parser to find argument boundaries. Except for `output`, all statement types must occur in "global context" from a d8m point of view.
That is, if they were statements of d8m, they'd be in global scope.
Each of these non-d8m statements must occur on its own line, so the initial keyword is the first token on the line,
and while it can occupy multiple lines, is must occupy entire lines.
With these restrictions, parsing the extension statements is simple.
The `output` statements occur in the function(s) that constitute the load model, so they'll never be in global context.

PerfsisMeasure accepts d8m string literals with minor restrictions. Thus, when writing the arguments to the special "perfsis" statements, you can use strings wrapped in double quotes or back quotes. As usual, back quoted strings are needed if newlines are embedded in the string, but cannot contain back quotes, as they have no quotation mechanisms at all.
The perfspec translator does not recognize d8m's string interpolation rules in double quoted strings (i.e., `#{...}` in strings), but you can use backslash to quote `n` (newline), `t` (tab), double quote, and backslash.

The first three statements have the obvious meanings; we'll start with `query`. The `query` statement body is a single string literal, a d8m fragment which becomes the query associated with the generated file (or files). The argument to query is subject to substitution (discussed with the `substitute` statement). Think of the argument to the `query` statement as the d8m code that sets up and launches your load model.

In perfsis terminology, a _parameter_ is a d8m identifier used by the load model, which you wish to vary when making measurements.
Each `parameter` statement defines a single parameter. Formally, it consists of the word `parameter`
followed by 3 or 4 comma separated string literals.
These arguments are:

1. a declaration that names the parameter in "symbol:type" format, similar to d8m declarations,
2. a documentation string,
3. a "range expression", and
4. an optional unit.

In the generated file, the symbol it declares becomes a global symbol of the indicated type.
In other words, the translated result of a `parameter` statement is a declaration of the parameter as a (global) d8m identifier.
(Remember that the `parameter` statement must occur in global scope.)
Parameters are allowed to have basic types: integer, float, string, or label. However, label and string types correspond to _categorical_ arguments, which are generally better served with `substitute` statements, described shortly. Therefore, numeric types are favored.

The range expression (argument 3) is a d8m expression that evaluates to type `list(T)` where `T` is the type
declared for the parameter symbol.
It defines the set of values you want for this parameter in the generated timing data file. For example, if you write

    parameter "size0:integer", "collection size", "[500, 5000, 50000]", "collection.count"

then the generated timing file will have rows with the `size0` parameter at each of the values 500, 5000, and 50000.
More specifically, the `size0` parameter generates 3 rows in the csv file; if there are no other parameters,
that's the entire csv file but otherwise, `size0` generates 3 rows for every value of every other parameter,
so the total number of rows is the product of the number of data points specified in the range expressions of all the parameters.
Furthermore, the load model code should use the `size0` symbol to control the relevant aspect of the
load model, which presumably relates to the size of some collection.

The optional _unit_ argument to parameters is used to help the front end know when different parameters are compatible.
Specifically, it judges them to be compatible just in case their units are identical as strings.
In other words, no intelligence is applied here &mdash; a unit is just a mnemonic that influences what things the front end offers to show together (so you can compare them). The compatibility of quantities that are counts (like the size of a collection) is more complex than quantities like length. Whereas the latter can always be compared, parameters that vary the memory size of some buffer probably shouldn't be compared with ones that vary the size of a collection.
Coming up with a nomenclature that causes sensibly comparable things to be effectively comparable is part of the art of using this tool well.

The `output` statement contains 2 comma separated arguments, both string literals. The first describes the output formally, while the second is a d8m expression that generates the output value when evaluated and printed. Outputs should always be numeric. The "formal" description (first argument) is analogous to the arguments to the parameter statement, consisting of a single string literal with 3 or 4 colon-separated fields: a brief name or phrase suitable as the label on a chart, a longer and less formal description suitable for a tooltip, the word "integer" or "float" to tell what kind of number to generate, and the fourth (optional) field an expression intended to be interpreted as a unit. For example

    output "insert:time to insert a set element:integer:nanosecond", "time.Since(t0)"

is a possible output statement. The driver would expect the output for this column to have integer format, would use the last argument in a `print` statement, and would pass all the arguments to the front end to help generate a useful and meaningful UI.

A perfspec file should not contain any print statements, the back end driver assumes that only output statements cause printing. You are welcome to use the logger package to debug your load models.

Whereas the other statement types help define and launch your load model, the role of `substitute` is a little different. It provides a very simple macro-like facility whereby perfsisMeasure creates multiple data files, one for each substituent. In this way, a single perfspec file, with a single load model and parameter set can be applied to several ways of implementing a given type, or varying other aspects of what you want to measure. For example, suppose you want to compare set implementations with `Gomap`, `olist`, and `splay` trees under a given mix of operations. To do this, you arrange to substitute the d8m expression that distinguishes each option for some arbitrary word, such as "SET". For example, suppose you've defined a type `T` and your query is

    var mySet = [set(T):]
    applyST(SET, mySet)
    runModel(mySet)

The `substitute` statement defines the expressions that cause this query to use each of the implementation options you want. It also provides a documentation string for each option and for the entire statement. Thus, its argument list consists of an even number of comma-separated arguments: (1) the _source_ -- a symbol to be substituted for; (2) a short description of what this substitution is for; (3) any number of pairs of string literals, where the first of each pair is the expression that is to be substituted or _substituent_, and the second a brief description of what it does, for use in the front end. Continuing our example:

    substitute SET, "set implementation,
        "Gomap(T, boolean)", "as map",
        "olist0(T)", "as ordered list",
        "splay(T)", "as splay tree"

(Note: we assume here that type `T` is ordered.)

This statement causes the perfsis back end to generate 3 different programs to run, one for each option, and a driver that generates 3 timing files with the names listed in a previous section.

A perfspec file can have multiple `substitute` statements. Each must use a different identifier as the source for substitution. The number of generated files is the product of the number of substituents for all of the `substitute` statements. The source symbol (`SET` in the example) should not occur in the file except in its substitution role, as perfspec "language" has no notion of overloading. (Note that the perfsisMeasure translator does correctly tokenize d8m code, including string literals. Therefore, occurrences of source identifiers within string literals do not trigger substitution.) As stated earlier, the argument to the `query` statement explicitly _is_ subject to substitution, which can be very useful, as shown in the above example.

PerfsisMeasure provides the following imports for perfspecs:

    import go "math/rand"
    import go "time"
    import go "flag"

Thus, when writing load models, you should assume these resources are available with qualifying identifiers `rand`, `time`, and `flag` respectively.

## Working Directory

In the last section we briefly described the "working directory" generated by perfsisMeasure for each perfspec and its contents. Here, we give a bit more detail. Each version subdirectory should have a number of .csv files equal to the product of the number of substituents for all the substitutions. Each of these files has the same number of rows and columns; these numbers are also easily calculated from the number of parameters, the number of measurements specified for each parameter, and the number of outputs.

We'll go through these numbers briefly, but first, a few words about the abstract situation. It's good to think of the data perfsisMeasure collects as a hypercube in a space whose dimensions correspond to what we call here parameters. Giving a value to each parameter identifies a vertex in this hypercube; each output (which is a measurement, in our case) is part of the contents of the indicated vertex. (You can think of the vertices as holding a tuple of output values, or you can think of each output as a separate hypercube whose vertices have a single value.)

Csv files encode one dimensional data in an obvious way, with each row holding one of the values for that dimension. More dimensions can be represented, either by enumerating multiple dimensions row by row, or by encoding the values of a dimension along columns. When encoding dimensions in rows, the values can be explicit, taking up a column, or implicit. Perfspec encodes its csv files by rows with explicit columns for the parameters. The dataTable module provided with d8m can read and write textual data tables with any combination of row and column encodings. It's used in the front end to format data for selected charts.

Thus, the number of columns in each csv file is `P+O` where `P` is the number of parameters and `O` is the number of outputs. That is, each parameter and each output gets a column. The number of rows is `P0*P1*...*Pk` where `Pi` is the number of values in the range spec of the _ith_ parameter. That is, each row is one point in the combination of measurements for each parameter value taken in lexicographic order with the first parameter varying slowest.

Each version subdirectory has one csv file for each combination of substituents. Files are named starting with `LM` and continuing with letters for substitutions and finishing with the `.csv` extension. So the number of letters after the `LM` equals the number of substitutions and each one gets `a` for the first value, `b` for the second, and so on. (So you can't have more than about 26 substituents which certainly shouldn't be a problem in practice.) For example, if there are no substitutions the single file is named `LM.csv`. One substitution with 3 values generates `LMa.csv`, `LMb.csv`, and `LMc.csv`. And so on. Two substitutions generates `LMaa.csv`, `LMab.csv`, ..., `LMba.csv`, and so on.

Meta data is in two json encoded files named "frontend.json" and "pfsData.json. Frontend.json is managed by the front end while pfsData is managed by the back end. Each json file encodes a single javascript object, which maps directly to a d8m entity. The d8m definition of what's in pfsData is

    val Parameter = tuple(ident, explanation, unit: string, elements:list(string))
    val Output = tuple(ident, explanation, unit: string)
    val Cubedef = tuple(ident, description: string, params:list(Parameter), outputs:list(Output))
    val Kvpair = tuple(key:string, value:list(string))
    val PfsData = tuple(cube: Cubedef, substns: list(Parameter), contexts: list(Kvpair), version: integer)

The PfsData type describes everything the back end manages: the logical hypercube defined by the parameters and outputs, the substitutions, and the versions. The contexts attribute takes the form of a dictionary whose keys are arbitrary identifiers that map to lists of strings. These lists are all the same length and contain an element for each version. Keys that aren't assigned for a given version get the empty string in the corresponding position of contexts for the relevant key. This is logically equivalent to a string-string dictionary per version with possibly distinct sets of keys. The version attribute gives the number of versions currently defined.

The corresponding definitions for frontend.json are discussed in an [upcoming section](#visualizing-data-with-perfsisshow).

## A Note on Random Numbers

We mentioned earlier that the go package `math/rand` is automatically imported into generated load model files. There is one thing you need to do however, since version 20 of go changed the behavior of that package, making it impossible to get identical pseudo-random sequences across runs without explicit code in your program. This section describes what happens under the covers, so you can get exactly what you want with a minimum of effort.

Let's assume the load model in your perfspec uses pseudo-random numbers and that you've chosen to use `math/rand` as the PRNG for these. What's the initial state of this generator? Since go version 20, each time a process is run using the default generator provided in `math/rand` its initial state is different, so you get a different sequence of numbers. If you only care about the randomness of this sequence, that's fine but if you prefer things a bit more deterministic, you can't use the default generator. (The same is true if you wish to use the `-r` flag of `pfsdriver`, documented in the next section.) It's useful to know that the `LM*.d8m` files that perfsisMeasure generates have a flag that sets a value named `randSeed`, and that this flag defaults to 1.

Accordingly, my suggestion is as follows. As the first line of your load model write

    var randx = rand.New(rand.NewSource(randSeed))
This sets `randx` to a non-default generator seeded with `randSeed`. Thenceforth, instead of writing `rand.Intn(2000)` or whatever variant of that you might use to get the default generator, write `randx.Intn(2000)` instead. 

If you wish to use a completely different PRNG, you can still use `randSeed` to initialize it. 

# Running perfsisMeasure

The perfsisMeasure program runs with a single argument, the name of a perfspec file. It reads and translates the file, and if successful, it creates and populates a working directory, as discussed in earlier sections. Note that you can re-run the program, to fix bugs for example, without ill effect. Once the working directory is properly created, the next step is to cd to the working directory and run the generated program named `pfsdriver`, which is customized, from a template, for this particular perfspec. The driver has several options accessed with flags: one for the initial run, others to create new versions. The options are listed in the following table:

| flag      | meaning                                                  |
| :-------- | :------------------------------------------------------- |
| -c=string | context: details below                                   |
| -i        | import files in remaining arguments                      |
| -v=N      | write to version N (must exist)                          |
| -r=string | random seed: details below                               |

The string value for the -c flag can take two forms. If it is bracketed with `[...]` then it's interpreted as a literal list of key-value pairs -- the contents inside the brackets should be a comma separated list of fields separated by colons, as in

    "[compiler:version 0.6.3,runtime:a1.medium,...]"

Spaces around keys and values are trimmed. If the string value to -c does not start with a square bracket, it's interpreted as a filename. The file is opened and read, its contents should be the context information in the same format as just given. That is, the file should start and end with square brackets, with comma separated fields of colon separated pairs.

The steps to run a perfspec named `X` are summarized in the following code segment:

    perfsisMeasure X.perfspec
    cd wd.X
    ./pfsdriver -c "[compiler:version a.b.c,runtime:a1.medium,...]"
This doesn't account for the fact that your perfspec might have bugs... Adapt accordingly.

It's worth noting that `pfsdriver` works by running each load model program on each set of parameter values. For example, if you have a substitution with 2 substituents, you'll get files `LMa.d8m` and `LMb.d8m`. If the perfspec has 2 parameters with 5 and 7 elements, the driver will compile `LMa.d8m` and run the resulting binary 35 times to cover all combinations of parameter values; similarly for `LMb.d8m`. The intention of this design is to ensure that each measurement is maximally independent of the others, but it's worth knowing that every measurement is done with a fresh process. 

Version management will be more effective if you choose a consistent set of keys for each perfspec. The -v=N flag lets you rewrite a version; -c is not needed in this case, unless you are updating some of the context info. This is useful if you change the perfspec in some way after running pfsdriver. Then, you run perfsisMeasure and then

    pfsdriver -v=0

to regenerate the data with the changes. 

It's not really possible to change the number of parameters, outputs, or substitutions in a perfspec once it has associated data. This is true because these factors define the form and meaning of the csv files that hold the data, as well as their names and multiplicity. More abstractly, these factors define the hypercube describing the structure of the data. For this reason, I like to call them the perfspec's _hypershape_. 

When you run perfsisMeasure, it checks whether the working directory already exists, and if so, it reads the existing `pfsData.json`. From this it learns the hypershape of the existing data. If the perfapec is now incompatible, it complains and exits. Therefore, if you decide to change any of the quantities making up the hypershape, you should first delete the data and other files. Or simply delete the working directory and regenerate it from scratch.

The -i flag creates a new version by importing csv files from somewhere else. Normally, pfsdriver takes no arguments, only flags, but in the -i case there should be either one argument naming a directory where the proper .csv files reside, or a list of .csv file names. The names and shapes of these files must conform to the perfspec's parameters and outputs. You should include a -c flag in this case as well, otherwise the new version will have no context information.

The -r flag lets you control the seed for the random number generator in go package "math/rand" and simultaneously lets you do multiple runs of the load models with different random seeds in order to create data that you can analyze statistically. Specifically, the string argument to -r consists either of a literal list of numbers (enclosed in square brackets and separated by commas), which are interpreted as random seeds, or of a single number, which is interpreted as the number of runs to do (with unspecified seeds). Note that you can provide a value of 0 to skip setting the seed in "math/rand".

## Practical Considerations

It can be difficult to measure the timing of exactly what you want. Modern CPUs are incredibly complicated, and quite good at scheduling independent parts of computations concurrently. The `time.Now()` and `time.Since()` functions take time to run. Attempts to get nanosecond level timing measurements are unlikely to be reliable. Use averages and consistency checks. 

# Visualizing Data with PerfsisShow

PerfsisShow is an interactive UI implemented with webgen that runs on localhost, port 3001. See the documentation on webgen for background information. The distributed form of perfsisShow is a shell script that runs a server made with webgen's `-deploy` flag. The server expects a `build/` directory in its working directory for the static files it serves, and a single argument naming a directory with one or more working directories with perfsis data. On startup, the server, stimulated by an `onLoad` transition from the client, reads and verifies these directories, and sends their names to the client for presentation in the UI.

The client initially presents a screen with a menu bar whose buttons are labeled Manage and Visualize, a dropdown selector labeled "Perfspecs...", and a message telling you that "you can't do anything here until you select a perfspec". Accordingly, select from the dropdown selector that was populated by the `onLoad` transition mentioned in the last paragraph. (If there's nothing in the selector, verify that the directory you gave to the server contains `wd.*` directories.) The menu bar highlights Visualize and the screen updates to show the first stage of selecting parameters to visualize something from the selected perfspec data.

As shown in the menu bar, the whole UI divides into two major parts for management and visualization. We'll describe Manage first, since some of the options arising in Visualize are created and managed there. Note that in many cases, you'll never use the Manage screen but it's useful to understand what it does.

## The Manage Page

In an [earlier section](#working-directory) we described the hypercube concept behind the data generated by perfsisMeasure, how it's mapped into csv files in a working directory generated by perfsisMeasure, and the shape of the json file describing all this. We mentioned that there's another json file, frontend.json, that's managed by perfsisShow. The Manage page is where this happens. We'll start with the concepts, and discuss the format of frontend.json as well.

First, perfsis supports a concept of _version_ which means variants of "the same" perfspec run under varying conditions. The meaning of "variants" is purposefully vague but it's meant to cover things like running benchmarks on different hardware configurations, different versions of a compiler, with different optimization flags, different STMappings of various entities, different versions of the benchmark itself, and so on. Versions are numbered starting from 0. It's intended that you document them with _context information_ as described [earlier](#running-the-back-end). Context information takes the form of _key-value_ pairs; if you choose a consistent set of words for keys, you'll get a better experience in the UI but this format aligns with the design intent that versions/variations can cover a broad set of needs.

As we'll see in [visualization](#the-visualize-page), a key driver for setting up a chart is to assign values to each of the dimensions of the hypercube that a given perfspec represents. The front end manages several items that further generalize the chart setup options.

The most straightforward of these is saved charts. On the Visualize page you can name and save the chart you're looking at. This saves the selected parameters (and graphical settings) and a Saved Charts dropdown selector in the setup part of Visualize lets you recall these charts easily. Another feature of saved charts is covered below.

The other two features are called _version sets_ and _externals_. A _version set_ is a subset of all the available versions. In the visualization context, a version set adds a dimension to the hypercube that the chart selects from. For example, if versions 5 and 6 result from running two distinct compiler optimization settings, you can compare these by creating a version set from them, then selecting it as a Multiseries in the chart generation dialogue. Version sets are always _categorical_ (not numeric). The Manage page shows you the existing version sets and lets you create, edit, and remove them.

External data arises from a source other than perfsisMeasure. As such, the hypercube of that data is usually simpler and, although there must be comparable dimensions, the values of their elements needn't be the same. External data always comes from a single csv file, which means there neither substitutions nor distinct versions. External data can represent anything you might want to include in a chart that also has data from the perfspec &mdash; performance data for a different programming language, data derived from a theoretical analysis of the algorithm, etc. The Manage page shows you the existing externals and lets you edit or remove them, and create new ones. The creation process starts with identifying the csv file containing the external data; the server reads this file and sends it to the client which asks you to fill in relevant information about the columns and any rows to exclude.

The fronend.json file also keeps track of which version is "preferred" and hence, the default one shown on charts. The Manage page lets you change this. To summarize, the relevant d8m type definitions for the front end follow:

    val ExternalData = tuple(cube: Cubedef, context: list(Kvpair), accessor: string)
    val VersionSet = tuple(ident, description: string, elements:list(string))
    val Frontend = tuple(
        externals: list(ExternalData),
        vsets: list(VersionSet),
        savedCharts: list(ChartDesc),
        preferredVersion: integer)

## The Visualize Page

When you've selected a perfspec, the Visualize page presents as a three stage state machine that you use to progressively define a chart. The stages are summarized by questions:

- What's in X?
- What's in Y?
- Other Cube Dimensions (and more)

Before we get into the details of how you define a chart by answering these questions, let's talk a little bit about the general capabilities of the charting in perfsisShow. It can show you one chart at a time, with a single X and a single Y axis. We'll say a _dataseries_ generates one line, or a set of points or bars. Your chart can show one dataseries or multiple dataseries, from multiple outputs or all the elements of some dimension (such as a substitution). There's also a way to _animate_ the chart so that by clicking on a control you move among different related datasets.

In addition to all of this, you can include external dataseries. And you can generate numeric labels showing relative values of two dataseries, either as ratios or differences, selected from different substitutions. In sum, the options for charting are quite rich.

We use the standard charting terminology of _abscissa_ and _ordinate_. The abscissa is plotted along the X axis. In a performance analysis it will commonly be the size of the input (the `N` in your `O(f(N))` notation). Every parameter of your perfspec's hypercube is listed. But it can also be substitutions, any version set, the _outputs_, or all the versions. Whereas parameters are numeric, all of these others are categorical. If the abscissa is categorical, its elements will be labeled along the x axis; if numeric, the charting software chooses the x axis labeling. 

The _ordinate_ is plotted on the Y axis, smaller values at the bottom. It is scaled to accomodate all values; if you're using the animation feature ("clickable") it's scaled for all possible click states, which can cause surprises. There are controls in the setup to specify min and max ordinate values. 

__What's in X__ lets you select the abscissa. Most commonly, you'll select a parameter, but it can also be a substitution or version. Once it's selected, the chart generation UI shows you the next question, namely __What's in Y__. You answer this question by checking one or more of the outputs listed at the bottom of the newly visible controls. Above that list are a few controls that affect visual form: radio buttons for line vs bar vs scatter plots, a "HueSlider" that sets base colors, radio buttons for the thickness of lines, and controls to set min and max Y values. 

It may happen that you want to select the outputs as the abscissa. It's possible to create a perfspec with no parameters; in this case, there's little choice for the abscissa but the outputs. In a no-parameters perfspec, this would be essentially a timeseries documenting the timing of a sequence of events: `E0` at `t0`, `E1` at `t1`, and so on. If there are multiple outputs, then `outputs` is an option for the abscissa; if selected, all of the outputs are auto-selected by default. 

A quick word about the HueSlider. PerfsisShow uses an algorithm to generate "similar" colors when you choose to have all the values of a given dimension (such as a substitution) shown together. (You select this in the next step with the __Multiseries__ option.) The algorithm uses a color space known as HSL &mdash; Hue, Saturation, Lightness. All the lines in your set have the same hue; the sequence goes from quite dark to increasingly light. By moving the slider, you set the hue (which is shown in the slider bar). 

Since the elements of a Multiseries are distinguished by colors, we use shapes to distinguish multiple outputs. Thus, if you select multiple outputs, they'll be shown for Line charts with different dashing patterns, or for dot (scatter) charts with different dot shapes. Thus, color variations unambiguously depict different elements of a Multiseries. The generated legends label these dimensions.  

After you select at least one output, you will generally get more questions and options, whose form and number depend on what you've selected and what options are available. The main question is labeled __Other Cube Dimensions__. This is a list of dropdown menus, each labeled with the name of a dimension (parameter or substitution) that _isn't_ used for the abscissa. Since it isn't used, some value has to be established for each such dimension, and the dropdown menu controls show you what the current value is, and gives you the option to change it. Usually, two options will be present: __Multiseries__, and __Clickable__. The first shows all the values at once in different colors; the second shows one at a time with a control that lets you quickly change one to another. The Multiseries option won't be shown for dimensions that have too many values, since 10+ lines in a chart of this type is a recipe for confusion rather than clarity. 

Since at most one dimension can be Multiseries and at most one can be Clickable, selecting either of these for a dimension will disable it from any other dimension.

Besides __Other Cube Dimensions__ there is a button labeled __Compare?__. It's enabled when it's possible to compare two dataseries. This is defined to be 
1. when you've selected one dataseries and it's possible to select another by varying substitution values
2. when you've already selected exactly two dataseries, either as two outputs or one plus an external, which is covered shortly. 

Clicking on __Compare?__ (when it is enabled) opens a modal dialogue consisting of a dropdown menu labeled "Compare with:", some radio buttons listing various options involving x and y, and a dismiss button labeled Done. If you select one of the x-y options and click Done, your chart will show two dataseries and one of them will be labeled with numbers comparing its values with that of the other dataseries. These numbers can be ratios or differences. If ratios, they may show up as percentages (like `77%`) or scaling factors (like `2.4x`). Attention: you won't be able to click on a radio button until you've selected a second dataseries with the dropdown menu.

If external dataseries are defined for this perfspec, they'll be presented just above the `Show Chart` button. Below a label that says __Add Externals?__ is a list of all the external dataseries that have been added to this perfspec. If you select one, some controls will pop up in a second column of the page to let you select outputs, hue, and thickness for the external dataseries. Normally, you'll want to set either hue or thickness to visually distinguish the external dataseries from whatever you're charting from the perfspec itself.

An external data source is a single csv file that can have multiple outputs, and hence multiple dataseries, but that should not have multiple parameters. The chart generator assumes the single parameter of this csv file is compatible with the selected abscissa. It is _not_ necessary for the values to line up, or for the ranges to be identical. The chart generation software currently shows the entire range for the abscissa of all sources. There is a current limitation of a single external data source per chart. 

