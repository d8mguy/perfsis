## Perfsis: D8m Performance Analysis

Perfsis provides a way to generate and structure performance data, and to visualize the results. See the documentation for details on how things work; there are examples of perfspec files in `tests/data`.

Note that although `perfsisMeasure`, which generates data, is only for d8m programs, `perfsisShow`, the visualisation part, can collect and show data from other sources, such as performance benchmarks for other programming languages. You'll find details in the documentation.

Installing creates executables named `perfsisMeasure` and `perfsisShow`. The first is a binary and the second a shell script. The basic types that `perfsisMeasure` uses to organize data are exported in a module named `types`. Accordingly, installation also adds a `perfsis` prefix to the modules DB so you can import `"perfsis/types"` in your own d8m programs when you need to use perfsis data programmatically. There is also a framework for testing, described in a later section.

### Installation

The distribution includes a `build` directory generated by the React platform, and the webgen spec that works with this. Make install compiles `perfsisMeasure`, and runs webgen to create a deployable version of the server which is names `perfsisShow`; it moves both to `/usr/local/bin`.

You use `perfsisMeasure` to create performance data from `.perfspec` files, and `perfsisShow` to visualize that data. Running `perfsisShow` starts a web server on port 8081, so you need to open a browser window on that port to interact. Invoked with no arguments, perfsisShow looks in the current working directory for perfspec data; it also accepts an argument with a directory:

    perfsisShow ~/.../myPerfsisData/
Note that if `perfsisShow` doesn't find any perfspec data in the directory it points to, the dropdown menu labeled `Perfspecs...` is simply empty; no other error indication is given. There is at present no way to change the directory interactively; exit and restart with a different directory to do this; you should also refresh the browser when doing this. 

Note that as-distributed, `perfsisShow` runs on port 8081, so the correct browser address is `localhost:8081`. To change this, you will need to set up the development environment, run webgen with a different port number, run build again, and arrange for perfsisShow to use the new binary and build directory.

I've tested perfsisShow on chrome and firefox. Some visual appearances are slightly different but the behavior seems to be identical on both.

### Available Data

The `perfspecs` directory in the demosite repository is a good place to point `perfsisShow` until you've generated some perfspecs of your own. Note that it expects such data to be local, so you'll need to download the data first. 

### Tests

The `tests` directory contains code for a form of test for `perfsisShow`. We'll call it `perfsisTest`, although as shown in a moment, you run it "manually" &mdash; there is no executable with that name. Testing interactive, visually oriented software is, of course, a challenge for automation, and `perfsisTest` doesn't try for automated testing. What it does is provide a kind of benchmark for the expected behavior of `perfsisShow`. In fact, a perfectly good description of `perfsisTest` is that it's a slideshow of charts generated by `perfsisShow` with instructions for how to obtain them. 

Install perfsisTest from this directory with 

    make tests
It runs on port 8082, so to run it you type

    cd perfsis/tests/build
    ./testSpecSrvrPort8082 ..
and open a browser tab at `localhost:8082`. Note the `..` argument to the server, which tells it where to find the slides and data. 

Let's stipulate that there's no reason to use `perfsisTest` as a test platform unless you're modifying `perfsisShow`, and more particularly, the chart generation software therein. If this doesn't describe you, you might still be mildly interested in `perfsisTest` because as a slide show with decent coverage of the chart generating options of `perfsisShow`, you could use it to learn more about how `perfsisShow` works.

As a test platform, the idea is to run `perfsisTest` in parallel with `perfsisShow`, pointing the `perfsisShow` server to the local data directory, i.e. `perfsis/tests/data`. For each slide in `perfsisTest`, you follow the instructions to generate the chart in `perfsisShow`, then compare (by eye) the generated chart to the one in the test. If they match, the test passes. 

It's worthwhile to understand that `perfsisTest` uses a symbolic description of the chart to create the chart it shows, not a screen capture. If you change the component that calls the recharts.js package, `perfsisTest` will continue to use the as-released version since it's linked into the deployed build, so the charts will presumably not match. If you recompile `perfsisTest`, it will pick up the latest version of the changed component, so its charts should again match those in `perfsisShow`. 

A caveat: if you change the definition of `ChartData` or `ChartDesc`, or of types used within these (such as `ChartPart`), you'll no longer be able to do a direct comparison, because the new `perfsisShow` can no longer read the data in `perfsis/tests/data`. Note that you wouldn't do this only in `perfsisShow`; you'd also do it in `perfsisMeasure` &mdash; your data format has changed. Thus, if you re-run `perfsisMeasure` on the `X.perfspec` files in `perfsis/tests/data` to generate data in the new format, then the test paradigm should work again. 




